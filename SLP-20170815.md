### 4.9 语言建模的进阶问题
#### 4.9.1 Kneser-Ney插值算法
基于绝对折价(absolute discounting)方法  
基本思想是基于单词出现的不同上下文数量来进行估计。  
插值KN算法比回退KN算法的表现更好  
所有的插值模型都可以用回退模型来表示  
#### 4.9.2 聚类N-gram
增加了词类信息的N-gram模型可以更好地应对训练数据中的稀疏问题  
IBM聚类是硬聚类(hard clustering)的一种，硬聚类指的是每个词只能属于一个类  
聚类N-gram可用于两种情形：   
1. 在对话系统中，领域特定的词类通常是人工定义的。
2. 对语料中的单词进行聚类来推导单词类型。但是单词词性这种类别并没有太大帮助。

#### 4.9.3 语言模型适配和Web
#### 4.9.4 长距离上下文信息的使用
基于主题的语言模型，训练不同主题的语言模型，然后混合在一起。  
skip N-gram模型
### 4.10 信息论背景
困惑度是概率的标准化(normalized)版本  
信息论教材(Cover and Thomas 1991)  
困惑度是基于信息论中交叉熵的概念。熵是信息的测度，在SLP中很重要。它可以作为评估一个特定语法的信息量的指标，或评估一个给定语法与一个给定语言的匹配程度，或评估一个给定N-gram语法预测下一个单词的能力。  
马尔可夫模型，即N-gram模型是平稳的，但是自然语言不是。下一个单词的概率可以依赖于任意距离的上下文。因此统计模型只是对自然语言的正确分布和熵的逼近。
#### 4.10.1 交叉熵
交叉熵的值越小，模型对分布的逼近越准确。
交叉熵与困惑度的关系
### 4.11 英语的熵
##### 为什么要估计英语的熵？
* 可以为将来的模型评估给出明确的下界
* 基于熵可以帮助我们了解语言中哪个部分提供了最多的信息(例如，英语的可预测性主要基于单词顺序？语义？构词？还是语用？)这可以帮助我们在进行语言建模时把注意力集中在重要的点上。   
香农提出了字母熵(per-letter entropy)的计算方法
### 语言模型的发展历史和现状
